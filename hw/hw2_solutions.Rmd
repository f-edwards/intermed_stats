---
title: "HW 2 Solutions"
author: "Frank Edwards"
date: "2/9/2021"
output: html_document
---

```{r, include = F}
library(tidyverse)
```


### 3.1 

```{r}
### set up the data as a data.frame()
surv_dat<-data.frame(age = c("18-29", 
                             "30-44",
                             "45-64",
                             "65+"),
                     support = c(0.5,
                                 0.6,
                                 0.4,
                                 0.3),
                     pop = c(200,
                             250,
                             300,
                             250))
```

A weighted average is computed as the sum of the product of the value times the population size in each strata divided by the total population.

\[\textrm{weighted average} = \frac{\sum_j N_j\bar{y_j}}{\sum_jNj}\]

```{r}
### easy in R once in data.frame()
sum(surv_dat$support * surv_dat$pop) / sum(surv_dat$pop)
### or in tidyverse
surv_dat %>% 
  summarize(wt_avg = sum(support * pop) / sum(pop))
```

### 3.6 

#### a. 

using $y = a + bx$ to describe the linear transformation

Because we know that $sd_y = 1.5 sd_x$, $b$ must be equal to 1.5. We also know $\bar{y} = 100, \bar{x} = 35$, so we can solve for $a = y - bx$ to complete the equation.

For a linear transformation with new mean equal to 100 and sd equal to 15, we can rescale the test as

\[y = 47.5 + 1.5x\]

#### b.

We can simply map $x \in [1,50]$ (that funky symbol means is a member of the set) to the equation above

```{r}
test_dat<-data.frame(x = 0:50)
test_dat<-test_dat %>% 
  mutate(y = 47.5 + 1.5 * x)
```

The range of possible values of the rescaled test score $y$ is `r range(test_dat$y)`

#### c. 

Here's the line showing x and y. It's got a y intercept of 47.5, and a slope of 1.5

```{r}
ggplot(test_dat,
       aes(x = x, y = y)) + 
  geom_line()
```

### 4.1 

```{r}
n<-500
y_t<-0.5
y_c<-0.4
```

The estimated treatment effect is simply $y_t - y_c = 0.1$. The standard error of a difference is computed as $\sqrt{se^2_t + se^2_c}$. For a proportion (or any binomial variable), the standard deviation is defined as $sd = p(1-p)$ and the standard error is defined as always as $\sqrt{sd/n}$ 

```{r}
### standard deviations
sd_t<-sqrt(500* 0.5 * (1-0.5))
sd_c<-sqrt(500* 0.4 * (1-0.4))
### standard errors
se_t<-sqrt(sd_t/n)
se_c<-sqrt(sd_c/n)

se_diff<-sqrt(se_t^2 + se_c^2)
```

So our treatment effect is `r y_t - y_c` and standard error of the treatment effect is `r se_diff`.

### 4.3 

```{r}
### define the variables
p1<-0.3
p2<-0.4

n<-20

xbar_1<-p1*n
xbar_2<-p2*n
```

We can treat `xbar_1` and `xbar_2` as the means of binomial random variables $x_1$ and $x_2$, which is the sampling distribution for the number of shots each player makes when they take 20 shots. From this, we can compute the standard deviations for $x_1, x_2$ as binomial random variables with $n = 20$. 

$$x_1 \sim Binomial(20, 0.3)$$

and player 2's distribution as

$$x_2 \sim Binomial(20, 0.4)$$
We can use the logic of a hypothesis test to evaluate the probability of the better shooter making more baskets with the null hypothesis $x_2 - x_1 >0$.

What is the probability of observing p2 make more baskets than p1?

Let's first compute the distribution of the difference as

```{r}
sd_x2<-sqrt(n * 0.4 * (1-0.4))
se_x2bar<-sqrt(sd_x2/n)

sd_x1<-sqrt(n* 0.3 * (1-0.3))
se_x1bar<-sqrt(sd_x1/n)


diff_means_se<-sqrt(se_x2bar^2 + se_x1bar^2)
```


$$\bar{x}_2 - \bar{x}_1 \sim N(2, 0.46)$$

So what is the probability of observing a value less than zero in a Normal distribution with mean 2 and sd 0.46?

Let's first compute a test statistic (z), by translating the null hypothesis into standard deviation (z-score) units. This z-distribution is $N(0,1)$

```{r}
z_null <- 
```

So we now know that zero falls `z_null` standard deviations away from the mean of the difference, or $2/3$. So what's the probability that we would observe a value $-2/3$ standard deviations away from the mean on a z-distribution, $N(0,1)$? We use the Normal cumulative density function to check this probability (the probability of observing something less than or equal to $-2/3$ on a Normal (0,1)).

```{r}
pnorm(-2/3, 0, 1)
```

So the probability that the better player makes more baskets on 20 shots is

```{r}
1 - pnorm(-2/3, 0, 1)
```

### 4.7 (trick) 

Using the adjustment when $p = 0$, $\hat{p} = \frac{y+2}{n+4} = \frac{2}{54}$. The standard error is then $\sqrt{\hat{p}(1-\hat{p})/(n+4)}$, and the 95 percent confidence interval is roughly $\hat{p} \pm 2se$.

```{r}
p_hat<-2/54
se_phat<-sqrt(p_hat*(1-p_hat) / (54))

p_hat
se_phat
## CI
p_hat - 2 * se_phat
p_hat + 2* se_phat 
```

Truncating at zero (can't have a negative proportion), we estimate a 95 percent CI of about [0, 0.09]

### 5.3 

#### a.

Use the binomial PDF

```{r}
dbinom(3, 10, 0.4)
```

#### b.

Simulate it

```{r}
### initiate empty data frame
shots_made<-data.frame(n = 1:10000, shots_made = rep(NA, 10000))
for(i in 1:nrow(shots_made)){
  shots_made$shots_made[i]<-rbinom(1, 10, 0.4)
}

sims<-rbinom(10000, 10, 0.4)

ggplot(shots_made, 
       aes(x = shots_made)) + 
  geom_histogram()

### compute proportion of 3s

shots_made %>% 
  summarise(prop3 = sum(shots_made==3)/n())

## Pretty close!
```


### 6.2 (hard)

```{r}
fake_data<-function(a, b, n, sigma){
  ### simulate uniform(0,100) x w
  x<-runif(n, 0, 100)
  ### simulate error
  error<-rnorm(n, 0, sigma)
  ### compute y
  y<-a + b * x + error
  ### fit model
  m_out<-lm(y~x)
  ### set up plotting data
  plot_dat<-data.frame(x = x, y = y)
  ### add plot, note geom_abline() draws a line with slope b and intercept a
  ggplot(plot_dat,
         aes(x = x, y = y)) + 
    geom_point() + 
    geom_abline(slope = b, intercept = a)
}
```

Check it out

```{r}
fake_data(a = 20, b = 2, n = 1000, sigma = 10)
fake_data(a = -500, b = 2, n = 5000, sigma = 25)
```


