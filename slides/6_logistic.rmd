---
title: "Logistic regression, 1"
author: "Frank Edwards"
date: "3/3/2021"
output: binb::metropolis
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(rstan)
library(rstanarm)
### to optimize stan model fitting for your computer, enable parallel processing
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())



theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "small")
```

# Logistic regression

## Read in the data for today

```{r size = "scriptsize"}
admissions <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
head(admissions)
nrow(admissions)
```

## Evaluate distribution of binary admission variable

```{r}
ggplot(admissions, aes(x = admit)) + geom_histogram()
```

## Properties of Bernoulli variables

If $y$ is an i.i.d. Bernoulli variable with probability $p$:

$$y \sim Bernoulli(p)$$
$$\Pr(y=1)=p=1-\Pr(y=0)$$
$$E(y) = \bar{y} = p$$ 
$$Var(y) = p(1-p)$$ 

## Summary of admit: What can we say about the probability of admission?

```{r}
mean(admissions$admit)
sum(admissions$admit==1)/nrow(admissions)
var(admissions$admit)
mean(admissions$admit) * (1 - mean(admissions$admit))
```

## How does GRE relate to admission?

```{r}
ggplot(admissions,
       aes(x = admit, y = gre)) + geom_point() + 
  geom_smooth(method = "lm")
```

## GPA?

```{r}
ggplot(admissions,
       aes(x = admit, y = gpa)) + geom_point() + 
  geom_smooth(method = "lm")
```

## Can we fit a model to predict admission?

```{r size = "scriptsize"}
m1<-lm(admit ~ gre + gpa, 
       data = admissions)
```


```{r echo = FALSE}
new_dat<-data.frame("gre" = mean(admissions$gre), 
                    "gpa" = seq(2,4, 0.01),
                    "rank" = 1)
yhat<-data.frame(predict(m1, interval = "confidence", newdata = new_dat))%>%
  cbind(new_dat)
ggplot(yhat, aes(x = gpa, y= fit, ymin = lwr, ymax = upr)) + 
  geom_line() + 
  geom_ribbon(alpha = 0.5)
```

## Let's try a different approach

```{r size = "scriptsize"}
m2<-glm(admit ~ gre + gpa, 
        data = admissions, 
       family = "binomial")
```

```{r echo = FALSE}
new_dat<-data.frame("gre" = mean(admissions$gre), 
                    "gpa" = seq(2,4, 0.01),
                    "rank" = 1)
yhat<-predict(m2,  
              newdata = new_dat,
              se.fit = TRUE)
yhat_df<-data.frame(fit = exp(yhat$fit)/(1+exp(yhat$fit)), 
                    upr = exp(yhat$fit + 2 * yhat$se.fit) / 
                      (1 + exp(yhat$fit + 2 * yhat$se.fit)), 
                    lwr = exp(yhat$fit - 2 *yhat$se.fit) / 
                      (1 + exp(yhat$fit - 2 *yhat$se.fit)))%>%
  cbind(new_dat)

ggplot(yhat_df, aes(x = gpa, y= fit, ymin = lwr, ymax = upr)) + 
  geom_line() + 
  geom_ribbon(alpha = 0.5)
```

## A generalized linear model

Our linear probability model was:

$$Pr(admit = 1) = \beta_0 + \beta_1GRE + \beta_2GPA + \beta_3Rank + \varepsilon$$

Our logistic regression model takes the form:

$$logit(Pr(admit=1)) =\beta_0 + \beta_1GRE + \beta_2GPA + \beta_3Rank$$

The logit function is our link between the linear predictor term $X \beta$ and the outcome $admit$. 

## The logit function

The logit function transforms a probability value on $[0,1]$ to a continuous distribution

$$logit(p) = log \frac{p}{1-p} $$

## The logit function

```{r}
p<-seq(0,1,0.001)
plot(log(p/(1-p)), pch = ".", p)
```

## Logistic regression is a GLM with a logit link

A generalized linear model with link function $g$ takes the form:

$$g(y) = X \beta$$

For OLS, the link function is the identity function $g(y) = y$

For logistic regression, the link function is the logit function

$$logit(y) = X \beta $$
$$y = logit^{-1}(X \beta) $$

## Defining logit and its inverse

$$logit(p) = log \frac{p}{1-p}$$
$$logit^{-1}(x) = \frac{exp(x)}{exp(x) + 1}$$ 

We can use these functions to transform values back and forth from our logit-linear scale and the probability scale. \pause

*Challenge:* create two functions in R. logit() and inv_logit() that will compute the logit for any $p$ and the inverse logit for any $x$.

## Defining logit and its inverse

```{r}
logit<-function(p){
  log(p/(1-p))
  }

inv_logit<-function(x){
  exp(x) / (exp(x) + 1)
}
```

## Challenge: practice with these functions

1. Create a tibble with values of p ranging from 0 to 1
2. Use mutate to add a variable called logit_p that is equal to $\textrm{logit}(p)$
3. Use ggplot to plot p and logit p

## Solution

```{r fig.height = 3}
#1.
new_dat<-tibble(p = seq(0, 1, by = 0.01))
#2.
new_dat<-new_dat %>% 
  mutate(logit_p = logit(p))
#3.
ggplot(new_dat,
       aes(x = p, y = logit_p)) + 
  geom_line()
```

## Challenge: practice with these functions

1. Create a tibble with values of x ranging from -10 to 10
2. Use mutate to add a variable called inv_logit_x that is equal to $\textrm{logit}^{-1}(x)$
3. Use ggplot to plot x and inv_logit_x

## Solution

```{r fig.height = 3}
#1.
new_dat<-tibble(x = seq(-10, 10, by = 0.1))
#2.
new_dat<-new_dat %>% 
  mutate(inv_logit_x = inv_logit(x))
#3.
ggplot(new_dat,
       aes(x = x, y = inv_logit(x))) + 
  geom_line()
```

## Motivation for logistic regression

1. Select a model that works for binary outcomes 
2. Preserve the linear structure for predictors
3. Map unbounded $(-\infty, \infty)$ linear predictors onto probability $(0, 1)$
4. Map expected probability into binary outcomes 

## Running logistic models in R: the glm() function

```{r}
m1<-glm(admit ~ gpa, 
        data = admissions, 
        family = "binomial")

m1_b<-stan_glm(admit ~ gpa, 
               data = admissions, 
               family = "binomial")
```

## What do these models mean?

```{r}
coef(m1_b)
```

This is the direct interpretation in terms of `admit`

\[Pr(admit_i=1|gpa) = \textrm{logit}^{-1}(\beta_0 + \beta_1gpa_i)\]

\pause

What does $\beta_1$ mean?

## What do these models mean?

```{r}
coef(m1_b)
```

\[Pr(admit_i=1|gpa) = \textrm{logit}^{-1}(\beta_0 + \beta_1gpa_i)\]

\pause

What does $\beta_1$ mean?

## Common interpretations

Because $\textrm{logit}(p_i) = \log(\frac{p}{1-p})= \beta_0 + \beta_1x_i$ \pause

We can interpret $\beta_1$ several ways

- Log odds: $\beta_1$ \pause
- Odds ratio: $e^{\beta_1 }$ \pause 
- Probability: $logit^{-1}(x) = \frac{exp(X \beta)}{exp(X \beta) + 1}$

## The challenge of interpretation 

The expected change in probability for $p$ for a 1 unit change in $x$ (or slope) is not constant!

```{r, fig.height = 3}
ggplot(new_dat,
       aes(x = x, y = inv_logit(x))) + 
  geom_line()
```

## The challenge of interpretation

1. Compute the expected probability of admission for a student with a 2.0 GPA \pause
2. For a student with a 3.0 GPA \pause
3. Assume the 2.0 and 3.0 student each bumped their GPA up by 0.5. How much does their expected probability of admission change?

## The challenge of interpretation

```{r size = "tiny"}
new_dat<-data.frame(gpa = seq(2, 4, by= 0.25))
new_dat<-new_dat %>% 
  mutate(log_odds = predict(m1, new_dat),
         probability = predict(m1, new_dat, type = "response"))
new_dat
```


## The challenge of interpretation: log odds scale

```{r size = "tiny"}
ggplot(new_dat, aes(x = gpa, y = log_odds)) + 
  geom_line() 
```

## The challenge of interpretation: probability scale

```{r size = "tiny"}
ggplot(new_dat, aes(x = gpa, y = probability)) + 
  geom_line() 
```

# Break

## Lab: Let's fit a more complex model

What else might predict admission?

```{r size = "tiny"}
head(admissions)
```

1. Write out a model 
2. Fit the model
3. Think about revising the model
4. Compare model fits
5. Interpret the model 

## Homework 5

Who was most (and least) likely to die on the Titanic? Use `~/hw/data/titanic.csv` for this one.

1. Write out a model 
2. Fit the model
3. Think about revising the model
4. Compare model fits
5. Interpret the model 

