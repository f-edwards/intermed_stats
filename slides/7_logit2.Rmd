---
title: "Interpreting logistic models"
author: "Frank Edwards"
date: "2/27/2023"
output: binb::metropolis
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(broom)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "small")
```

## Quick review

- Remember that logistic regression is a GLM with a logit link function \pause
- A GLM takes the form: $g(y) = \textrm{X}\beta$ \pause
- Logistic regression is the special case where g is the logit function: $\textrm{logit}(y) = \textrm{X}\beta$ \pause
- A logistic regression model returns $\textrm{X}\beta$ on the logit scale \pause
- How can we convert $\textrm{x}\beta$ to something useful?

## Let's return to the grad school admission example

```{r}
admits<-read_csv("./data/binary.csv")
summary(admits)
```

## Let's look at this as the distribution of the probability of admissions across the data

- First, fit an intercept-only logistic regression model

```{r}
m0<-glm(admit ~ 1, data = admits, family = "binomial")
m0_est<-tidy(m0)
```

- What does this model tell us? 

## What does this model tell us?

```{r size = "tiny"}
m0_est$estimate ## log odds
exp(m0_est$estimate) ## odds
exp(m0_est$estimate) / (1 + exp(m0_est$estimate)) ## probability
mean(admits$admit) ## mean admission probability
```

## Let's add a predictor

```{r}
m1<-glm(admit ~ 1 + gre, data = admits, family = "binomial")
tidy(m1)
```

## Predictions for all data points in M0 and M1 - what's going on?

```{r echo = F}
par(mfrow=c(1,2))
hist(predict(m0, type = "response"), main = "Intercept only")
hist(predict(m1, type = "response"), main = "With GRE predictor")
```

## Two linear predictors

Why do these generate such different predictions?

**Intercept only model (m0)**: $p(admit) = \beta_0$

**With GRE predictor (m1)**: $p(admit) = \beta_0 + \beta_1 GRE_i$

## To ease interpretation, let's scale GRE

Scale mean-centers and SD scales variables: $\textrm{scale}(x_i) = \frac{x_i - \bar{x}}{sd(x)}$

## Linear transformations of variables: mean-center and SD scale

```{r echo = F}
plot_dat<-admits %>% 
  mutate(gre_s = as.vector(scale(gre)))

par(mfrow=c(1,2))
plot(density(plot_dat$gre))
plot(density(plot_dat$gre_s))
```


## Re-estimate the model: much nicer to look at
```{r}
admits<-admits%>%
  mutate(gre_s = as.numeric(scale(gre)))
m1<-glm(admit ~ 1 + gre_s, data = admits, family = "binomial")
m1_est<-tidy(m1)
m1_est
```

## Interpret the model

```{r}
m1_est
```


Remember: $\textrm{logit}(y) = \textrm{X}\beta = \textrm{log}\left(\frac{y}{1-y}\right)$

So: $y = \textrm{logit}^{-1}(\textrm{X}\beta) = \frac{\textrm{exp}(\textrm{X}\beta)}{\textrm{exp}(\textrm{X}\beta) + 1}$ \pause

- What is $\beta_0$? \pause
- What is $\beta_1$?

## Refresher on exponentials

$$e^{y_1 + y_2} = e^{y_1} e^{y_2}$$ \pause

and 

$$e^{y_1 - y_2} = \frac{e^{y_1}}{e^y_2}$$ \pause

so how can we rewrite: 

$$\textrm{exp}(\textrm{logit}(y)) = \frac{y}{1-y} = e^{\beta_0 + \beta_1x_1} $$

## Non-linear relationships

On the log scale, $\beta_0$ and $\beta_1$ are related to y multiplicatively because

$$e^{\beta_0 + \beta_1x_1} = e^{\beta_0}e^{\beta_1x_1}$$

## Odds 

Odds are defined as the probability of the event occurring divided by the probability of probability of the event not occurring. To obtain odds in a logistic regression, we exponentiate both sides: 

$$\frac{y}{1-y} = e^{\beta_0 + \beta_1x_1}$$ \pause

The odds of $y==1$ are simply $e^{X\beta}$

## Odds ratios

The odds ratio is the ratio of two odds - or the proportional change in odds. We can obtain an isolated estimate for the relationship between $\beta_1x_{1i}$ and $y$ this way:

$$\frac{Odds(y |x_1 = 1)}{Odds(y|x_1 = 0)} =  \frac{e^{X\beta + \beta_1}} {e^{X\beta}} = \frac{e^{X\beta} \times e^{\beta_1}} {e^{X\beta}} =  e^{\beta_1}$$

The odds ratio can be interpreted as the change in odds of $y==1$ for a one-unit change in $x_1$. 

## Interpreting odds ratios

- Odds ratios appear convenient - $e^{\beta_1}$ is a percent change in $y$ for a one-unit change in $x_1$ \pause

How do they work?

## In our example: what do these figures mean?

```{r size = "scriptsize"}
new_dat<-c(1,0) # for scale(gre) == 0, mean score
odds_0<-exp(new_dat%*%m1_est$estimate)
odds_0
new_dat1<-c(1,1)
odds_1<-exp(new_dat1%*%m1_est$estimate)
odds_1
odds_1/odds_0 # odds ratio
exp(m1_est$estimate[2]) # exp(beta_1)
```

## Interpreting the odds ratio

The odds of admission are `r exp(m1_est$estimate[2])` times higher for a student with a GRE score one standard deviation above the mean than they are for a student with a mean GRE score. \pause

Any trouble you can anticipate here? \pause

## A visual example: the "effect" of 1 SD increase in GRE scores on Pr(admit==1)

```{r, echo = FALSE}
new_dat<-data.frame(gre_s=seq(-3, 0.8, by = 0.001))
p<-predict(m1, newdata = new_dat, type = "response")
new_dat2<-new_dat%>%mutate(gre_s = gre_s + 1)
p2<-predict(m1, newdata = new_dat2, type = "response")
par(mfrow=c(1,3))
plot_dat<-data.frame(p = p2, gre_s = new_dat2$gre_s, type = "P(admit)")
plot_dat<-plot_dat%>%
  bind_rows(data.frame(p = p2 - p, gre_s = new_dat2$gre_s, type = "P(admit|gre_s+1)-P(admit)"))
            
ggplot(plot_dat, aes(x = gre_s, y = p, color = type)) + geom_line()

```

## It is easy enough to work on the probability scale

To obtain predicted probabilities of the observed:

- p_hat<-predict(m1, type = "response")

## On the probability scale

```{r size = "scriptsize", fig.height = 2}
preds<-predict(m1, type = "response")
p_hat<-data.frame(gre = admits$gre,
                  p = preds)

ggplot(p_hat, aes(x = gre, y = p)) + 
  geom_line()
```

## The basic logic of prediction

1. Choose scenarios of theoretical interest
2. Define these in terms of "counterfactual" (fake) data
3. Plug these fake data into the linear predictor (regression equation)
4. Visualize!

## The basic logic of prediction

Reminder: our model is 

$$logit(p(admit_i)) = \beta_0 + \beta_1GRE_i$$
$$admit_i \sim Binomial(1, p)$$
```{r}
m1<-glm(admit ~ gre, data = admits, family = "binomial")
```

1. Choose scenarios of theoretical interest

Low GRE, average GRE, high GRE

## Define these scenarios in R

2. Define these in terms of "counterfactual" (fake) data

```{r size = "tiny"}
## Look at the distribution of the data to think about scenarios
mean(admits$gre)
sd(admits$gre)
```

\pause

Let's define scenarios at the mean, 1 SD below the mean, and 1 SD above the mean

```{r size = "tiny"}
fake_data<-data.frame(gre = c(
  mean(admits$gre),
  mean(admits$gre) - sd(admits$gre),
  mean(admits$gre) + sd(admits$gre)
))

fake_data
```

## Generating expected probabilities

3. Plug these fake data into the linear predictor (regression equation)

Because $logit(p(admit_i)) = \beta_0 + \beta_1x_i$, we can compute the expected probability of admission for a student with mean GRE scores as

```{r}
coef(m1)
### mean GRE scenario: linear predictor
exp(-2.9 + 0.0036 * 587.7) / (1 + exp(-2.9 + 0.0036 * 587.7))
```

## Generating expected probabilities

3. Plug these fake data into the linear predictor (regression equation)

The `predict()` function makes life very easy here

```{r}
## linear predictor
predict(m1, newdata = fake_data)
## probability scale (inverse logit)
predict(m1, newdata = fake_data, type = "response")
```

## Intrepretation through visuals

4. Visualize!

```{r size = "tiny"}
### set up our data frame with predictions for plotting
fake_data<-fake_data %>% 
  mutate(p_hat = predict(m1, newdata = fake_data, type = "response"))

ggplot(fake_data,
       aes(x = gre, y = p_hat)) + 
  geom_point()
```


